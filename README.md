# ğŸ’¼ Job Market Trends Analyzer & Resume Matcher
> **End-to-End Data Pipeline & AI-Powered Career Assistant**

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-App-red.svg)
![Docker](https://img.shields.io/badge/Docker-Container-blue.svg)
![Selenium](https://img.shields.io/badge/Selenium-Automation-green.svg)

## ğŸ“Œ Project Overview
This project is an **End-to-End Data Engineering & Science Portfolio** designed to bridge the gap between job seekers and market demands. It automates the collection of job postings, extracts key skill requirements using NLP, and provides a **Personalized Resume Matching System**.

**Key Features:**
*   **ğŸ¤– Automated Data Pipeline**: Scrapes real-time job postings (LinkedIn) using Selenium.
*   **ğŸ§  NLP Skill Extraction**: Parses unstructured Job Descriptions (JD) to extract tech stacks (e.g., "Python", "AWS", "Large Language Models").
*   **ğŸ¯ Intelligent Resume Matcher**: Parses user resumes (PDF), compares them against job requirements, and performs a Gap Analysis (Missing vs. Matched Skills).
*   **ğŸ“Š Interactive Dashboard**: Visualizes salary trends, top in-demand skills, and personalized job recommendations.

---

## ğŸ—ï¸ Architecture

The system follows a modern ETL (Extract, Transform, Load) pattern with a serving layer.

```mermaid
graph TD
    subgraph "Ingestion Layer"
        A[LinkedIn Scraper] -->|Raw HTML/Text| B(Raw Data Storage)
        style A fill:#f9f,stroke:#333,stroke-width:2px
    end

    subgraph "Processing Layer (NLP)"
        B -->|Read Raw Data| C[Skill Extractor]
        C -->|Regex & NLP| D[Structured Data CSV]
        subgraph "Resume Analysis"
             U[User Resume PDF] -->|PyPDF Parsing| P[Resume Parser]
             P -->|Skill Extraction| M[Match Engine]
        end
    end

    subgraph "Serving Layer"
        D -->|Load Jobs| S[Streamlit Dashboard]
        M -->|Gap Analysis| S
        S -->|Visuals & Recommendations| User
    end
    
    style S fill:#bbf,stroke:#333,stroke-width:2px
```

## ğŸ› ï¸ Tech Stack
| Category | Technologies |
|----------|--------------|
| **Language** | Python 3.9+ |
| **Data Collection** | Selenium Webdriver, BeautifulSoup |
| **Processing & NLP** | Pandas, NLTK, Regex, PyPDF (Resume Parsing) |
| **Visualization** | Streamlit, Plotly Express |
| **Infrastructure** | Docker, Docker Compose |

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Local Installation (Without Docker)

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/job-market-analyzer.git
    cd job-market-analyzer
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Run the Pipeline (Optional - if you want fresh data):**
    ```bash
    # 1. Scrape Data
    python src/ingestion/scraper.py
    
    # 2. Process & Extract Skills
    python src/processing/extractor.py
    ```

4.  **Launch the Dashboard:**
    ```bash
    streamlit run src/dashboard/app.py
    ```

### 2ï¸âƒ£ Running with Docker (Recommended)

Ensure you have Docker installed.

1.  **Build and Run:**
    ```bash
    docker-compose up --build
    ```

2.  **Access the App:**
    Open your browser at `http://localhost:8501`

---

## ğŸ“‚ Project Structure
```bash
job-market-analyzer/
â”œâ”€â”€ data/               # Local storage for data (Git-ignored)
â”‚   â”œâ”€â”€ raw/            # Raw HTML/JSON job postings
â”‚   â””â”€â”€ processed/      # Cleaned CSVs with extracted skills
â”œâ”€â”€ notebooks/          # Jupyter notebooks for EDA & Prototyping
â”œâ”€â”€ src/                # Source code
â”‚   â”œâ”€â”€ ingestion/      # Scrapers & API clients
â”‚   â”œâ”€â”€ processing/     # Cleaning, NLP, & Resume Matching Logic
â”‚   â””â”€â”€ dashboard/      # Streamlit Application
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ Dockerfile          # Docker image configuration
â”œâ”€â”€ docker-compose.yml  # Container orchestration
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # Documentation
```

## ğŸ‘¥ Author
*   **Shawn Nam**
*   **Contact**: 'https://www.linkedin.com/in/shawn-nam-b79614204/' | [tjr001136@gmail.com]
